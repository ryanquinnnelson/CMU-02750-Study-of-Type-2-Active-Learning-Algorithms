{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cooperative-newcastle",
   "metadata": {},
   "source": [
    "## Question 1. ZLG algorithm implementation (50 points)\n",
    "**You are to implement the ZLG algorithm for this problem.**\n",
    "- **We will use a subset of multiclass data where the label is a protein subcellular localization.**\n",
    "- **The 8 features are extracted from the protein sequence.**\n",
    "- **For this problem we are only using points with labels `MIT` or `NUC`.**\n",
    "- **A total of 673 data points have labels `MIT` or `NUC`. We start with the labels of only the first 200 data points (set `Y_k`). The other 473 points are in `Y_u`.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "announced-denial",
   "metadata": {},
   "source": [
    "\n",
    "# 1.0. Part 0. (15 points)\n",
    "**First, read the paper and answer the following questions.**\n",
    "### 1. What is the idea behind the ZLG algorithm (5 points)?\n",
    "\n",
    "#### Formal Version\n",
    "ZLG is a Type II active learning algorithm used for binary classification on a pool-based data access model. It \n",
    "labels instances based on expected risk, and it comes with no guarantees of performance or label complexity.\n",
    "\n",
    "The algorithm starts by building a weighted graph over a pool of labeled and unlabeled instances, with edges connecting \n",
    "instances which are deemed \"similar enough.\" ZLG uses the graph and known labels to construct a probability \n",
    "distribution over the unlabeled instances. It finds the optimal labeling solution for the distribution in closed-form,\n",
    "producing label predictions for all unlabeled instances.\n",
    "\n",
    "ZLG uses these predictions to calculate the expected risk of inferring labels for all unlabeled instances.\n",
    "The algorithm searches through the unlabeled \n",
    "instances to find the one which, when labeled, reduces the expected risk the most. ZLG queries the label for this instance and updates label\n",
    "predictions for all unlabeled instances based on the label. This search process is \n",
    "repeated until a defined stopping point, at which point we use the current label predictions to label all unlabeled \n",
    "instances.\n",
    "\n",
    "#### Informal Version\n",
    "*In layman's terms, ZLG starts with the idea that if we assume data has natural clusters, we can also assume that \n",
    "two data points which are close together are likely to have the same label. Under this theory, labeled instances \n",
    "surrounding an unlabeled instance \"influence\" what the label ought to be, with the amount of influence depending on \n",
    "how similar a labeled instance is to the unlabeled one.* \n",
    "\n",
    "*ZLG models this influence as a system of energy, with each data point radiating energy outward onto all of its \n",
    "neighbors. Data points with similar labels \"vibrate\" at similar wavelengths, creating resonance; data points \n",
    "with different labels vibrate at different wavelengths, creating dissonance. By considering the energies of all \n",
    "of the points, we can determine the energy of the overall system. ZLG leverages the well-studied principles of a system of energy (Boltzmann distribution) to efficiently estimate a \n",
    "labeling which produces a system with the least energy (i.e. least dissonance), given the current set of labeled instances. The estimates represents what the labels should be when considered as a system of energy.*\n",
    "\n",
    "*Given that this is an estimate, there is some risk that the predicted labels are wrong. Until its query budget is used up, ZLG repeatedly searches for the best instances to query to minimize this risk, updating its predictions along the way.*\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 2. What are the assumptions behind the ZLG algorithm (5 points)?\n",
    "- ZLG assumes that a Radial Basis Function provides a good measurement of the similarity between data points.\n",
    "- ZLG assumes that the labels follow a Boltzmann distribution\n",
    "- ZLG assumes that inferred labelings, given the labeled data, follow a Multivariate Gaussian distribution\n",
    "- ZLG assumes that expected risk is the best strategy for selecting points to label\n",
    "- ZLG assumes that all of the features have equal importance when calculating similarity\n",
    "- ZLG assumes we have a pool of data.\n",
    "- ZLG assumes we have binary labels.\n",
    "\n",
    "\n",
    "### 3. What are the pros and cons of the ZLG algorithm (5 points)?\n",
    "#### Pros\n",
    "- ZLG allows for very efficient label estimates and decisions for which points to label.\n",
    "- ZLG uses well-known techniques like the Laplacian matrix, which helps with comprehension of the algorithm and what it is doing.\n",
    "- ZLG uses expected risk as the query selection strategy, which is a more rigorous method than uncertainty.\n",
    "\n",
    "\n",
    "#### Cons\n",
    "- ZLG does not come with performance or label complexity guarantees.\n",
    "- ZLG makes a number of assumptions about the relationship of the labels to the data points, for mathematical convenience. If those assumptions do not hold, then the model may not perform well.\n",
    "- A lot of importance is put on accurately representing the similarity of two data points. If certain features of data points are more important than others, ZLG does not take this into account when calculating weights. This could lead to a less performant model overall.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neither-quebec",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "sporting-patrol",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial import distance_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import packages.zlg.zlg as zlg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "empirical-coach",
   "metadata": {},
   "source": [
    "### Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fitting-baseball",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape (673, 9)\n",
      "unique labels ['MIT' 'NUC']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.58</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.22</td>\n",
       "      <td>MIT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.43</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.22</td>\n",
       "      <td>MIT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.64</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.22</td>\n",
       "      <td>MIT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.58</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.22</td>\n",
       "      <td>NUC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.42</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.22</td>\n",
       "      <td>MIT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     X1    X2    X3    X4   X5   X6    X7    X8 Label\n",
       "0  0.58  0.61  0.47  0.13  0.5  0.0  0.48  0.22   MIT\n",
       "1  0.43  0.67  0.48  0.27  0.5  0.0  0.53  0.22   MIT\n",
       "2  0.64  0.62  0.49  0.15  0.5  0.0  0.53  0.22   MIT\n",
       "3  0.58  0.44  0.57  0.13  0.5  0.0  0.54  0.22   NUC\n",
       "4  0.42  0.44  0.48  0.54  0.5  0.0  0.48  0.22   MIT"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data/data.csv')\n",
    "print('shape',data.shape)\n",
    "print('unique labels', data.Label.unique())\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "increased-violence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(673, 8)\n",
      "(673,)\n"
     ]
    }
   ],
   "source": [
    "# filter out records without desired labels\n",
    "data_MITNUC = data.loc[data['Label'].isin(['MIT','NUC'])].values\n",
    "\n",
    "# split data into features and target, encode target classes as 0 or 1\n",
    "X = data_MITNUC[:,:8]\n",
    "y = LabelEncoder().fit_transform(data_MITNUC[:,-1])\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "prerequisite-outdoors",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 8)\n",
      "(200,)\n",
      "(473, 8)\n",
      "(473,)\n"
     ]
    }
   ],
   "source": [
    "# split data into first 200 records and remainder\n",
    "n_l = 200\n",
    "\n",
    "Xk = X[:n_l,:]\n",
    "Yk = y[:n_l]\n",
    "Xu = X[n_l:,:]\n",
    "Yu = y[n_l:]\n",
    "\n",
    "print(Xk.shape)\n",
    "print(Yk.shape)\n",
    "print(Xu.shape)\n",
    "print(Yu.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "multiple-mustang",
   "metadata": {},
   "source": [
    "# 1.1. Part 1 (5 points)\n",
    "**TODO:**\n",
    "- **Let's first construct the weight matrix W.**\n",
    "- **Use $t = 0$ and $\\sigma$ as the standard deviation of $X$.**\n",
    "- **Calculate the $D$ matrix and the Laplacian matrix (Delta) for use in other parts.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alleged-smile",
   "metadata": {},
   "source": [
    "#### Radial basis function (RBF):\n",
    "Calculates similarity between points $x_i$ and $x_j$ using squared Euclidean distance.\n",
    "\n",
    "$$w_{ij}=\\exp{\\left( -\\frac{1}{\\sigma^2}\\sum_{d=1}^m (x_{id} - x_{jd})^2 \\right)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "advisory-bhutan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# laplacian_matrix() implemented in custom Python package zlg\n",
    "# calculations are performed in Part 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controversial-principal",
   "metadata": {},
   "source": [
    "# 1.2. Part 2 (5 points) \n",
    "**TODO:**\n",
    "- **Now complete the subroutine to compute the minimum-energy solution for the unlabeled instances.**\n",
    "    - **Hint: Use the formula in page 38, Lecture 7.** \n",
    "- **The function also outputs one submatrix that we will use to select points to query.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yellow-credit",
   "metadata": {},
   "source": [
    "#### Minimum energy solution $f_u$\n",
    "\n",
    "$$f_u = -\\Delta_{uu}^{-1}\\Delta_{ul}f_l$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "vocational-electron",
   "metadata": {},
   "outputs": [],
   "source": [
    "# minimum_energy_solution() implemented in custom Python package zlg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hydraulic-attendance",
   "metadata": {},
   "source": [
    "# 1.3. Part 3 (15 points) \n",
    "**TODO:**\n",
    "- **We would like to query the points that minimize the expected risk. To do so, we want to be able to calculate the expected estimated risk after querying any point $k$.**\n",
    "- **The variable `Rhat_fplus_xk` refers to $\\hat{R}(f^{+x_k})$.**\n",
    "- **`fu_xk0` is $f_u^{+(x_k,0)}$ and vice versa for `fu_xk1`.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acting-platform",
   "metadata": {},
   "source": [
    "#### Expected risk:\n",
    "\n",
    "$$\\hat{R}\\left( f^{+(x_k,y_k)} \\right)=\\sum_{i=1}^n min\\left( f_i^{+(x_k,y_k)},1-f_i^{+(x_k,y_k)}\\right)$$\n",
    "\n",
    "where \n",
    "- $f^{+(x_k,y_k)}$ is a n x 1 vector of the updated minimum energy solution for n unlabeled points if point $k$ was labeled\n",
    "- $f_i^{+(x_k,y_k)}$ is the updated minimum energy solution of the ith unlabeled point if point $k$ was labeled\n",
    "\n",
    "***\n",
    "\n",
    "#### Expected estimated risk:\n",
    "\n",
    "$$\\hat{R}\\left( f^{+x_k} \\right)=\n",
    "(1-f_k)\\hat{R}\\left( f^{+(x_k,0)} \\right)\n",
    "+   f_k\\hat{R}\\left( f^{+(x_k,1)} \\right)$$\n",
    "\n",
    "where\n",
    "- $f_i^{+(x_k,0)}$ is the updated minimum energy solution of the ith unlabeled point if point $k$ was labeled with $y_k=0$\n",
    "- $f_i^{+(x_k,1)}$ is the updated minimum energy solution of the ith unlabeled point if point $k$ was labeled with $y_k=1$\n",
    "- $f_k$ is the current minimum energy solution of the kth unlabeled point\n",
    "\n",
    "\n",
    "***\n",
    "\n",
    "#### Conditional Distribution of all unlabeled nodes:\n",
    "\n",
    "$$f_u^{+(x_k,y_k)}=f_u+(y_k-f_k)\\frac{(\\Delta_{uu}^{-1})_{ \\cdot k}}{(\\Delta_{uu}^{-1})_{kk}}$$\n",
    "\n",
    "where\n",
    "- $f_u$ is the updated minimum energy solution of unlabeled points if instance $x_k$ was labeled with $y_k$\n",
    "- $f_k$ is the current minimum energy solution of the kth unlabeled point\n",
    "- $\\Delta_{uu_{. k}}^{-1}$ is the kth column of the inverse Laplacian on unlabeled data\n",
    "- $\\Delta_{uu_{kk}}^{-1}$ is the kth diagonal element of the same matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bulgarian-hepatitis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# expected_estimated_risk() implemented in custom Python package zlg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wrong-circumstances",
   "metadata": {},
   "source": [
    "# 1.4. Part 4 (5 points) \n",
    "**TODO:**\n",
    "- **Compute the above expected estimated risk for all unlabeled points and select one to query.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "expensive-matrix",
   "metadata": {},
   "outputs": [],
   "source": [
    "# zlg_query() implemented in custom Python package zlg\n",
    "# selection occurs in Part 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "western-gallery",
   "metadata": {},
   "source": [
    "# 1.5. Part 5\n",
    "**TODO:**\n",
    "- **Let's try querying 100 points. Which points are queried?** \n",
    "- **Compare with random queries and make a plot.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "disciplinary-translator",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test score_model()\n"
     ]
    }
   ],
   "source": [
    "def score_model(f_u, y_true):\n",
    "    y_pred = np.round(f_u)\n",
    "    \n",
    "    if y_pred.shape[0] != y_true.shape[0]:\n",
    "        raise ValueError('Arrays must be the same size to compare.')\n",
    "    \n",
    "    wrong = (y_pred != y_true).sum()\n",
    "    error = wrong / len(y_true)\n",
    "    return 1.0 - error\n",
    "\n",
    "\n",
    "print('test score_model()')\n",
    "a = np.array([0.51,0.7,0.1,0.2])\n",
    "b = np.array([1,0,0,0])\n",
    "assert score_model(a,b) == 3/4, score_model(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "failing-plymouth",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accuracy(accuracy):\n",
    "    plt.xlabel('Number of queries')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Number of queries vs Accuracy')\n",
    "    plt.plot(accuracy)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "short-perry",
   "metadata": {},
   "source": [
    "### ZLG Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepared-imaging",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time  \n",
    "# 2 min\n",
    "\n",
    "# copy data for use in this section\n",
    "Xk_zlg = copy.deepcopy(Xk)\n",
    "Yk_zlg = copy.deepcopy(Yk)\n",
    "Xu_zlg = copy.deepcopy(Xu)\n",
    "Yu_zlg = copy.deepcopy(Yu)\n",
    "\n",
    "# variables\n",
    "n_samples = X.shape[0]\n",
    "n_l = Xk_zlg.shape[0]\n",
    "labeled_zlg = [i for i in range(200)]\n",
    "unlabeled_zlg = [i for i in range(200,673)]\n",
    "\n",
    "# initialize components\n",
    "Delta = zlg.laplacian_matrix(X,t=0)\n",
    "fu,Delta_uu_inv = zlg.minimum_energy_solution(Delta,labeled_zlg,unlabeled_zlg,Yk)\n",
    "\n",
    "\n",
    "# run required iterations\n",
    "queried_zlg = []\n",
    "scores_zlg = []\n",
    "for count in range(100):\n",
    "    \n",
    "    # select unlabeled instance \n",
    "    query_idx = zlg.zlg_query(fu, Delta_uu_inv,n_l,n_samples)\n",
    "    queried_zlg.append(query_idx)\n",
    "    \n",
    "    # add instance to labeled set\n",
    "    Yk_zlg = np.append(Yk_zlg,Yu_zlg[query_idx])\n",
    "    Xk_zlg = np.append(Xk_zlg,[Xu_zlg[query_idx,:]],axis=0)\n",
    "    n_l += 1\n",
    "    labeled_zlg.append(unlabeled_zlg.pop(0))  # move first element in unlabeled to end of labeled list\n",
    "    \n",
    "    # remove instance from unlabeled set\n",
    "    Yu_zlg = np.delete(Yu_zlg,query_idx)\n",
    "    Xu_zlg = np.delete(Xu_zlg,query_idx, 0)\n",
    "    \n",
    "    # update Laplacian\n",
    "    Delta = zlg.laplacian_matrix(np.concatenate((Xk_zlg,Xu_zlg),axis=0), t=0)\n",
    "    \n",
    "    # calculate minimum energy solution for remaining unlabeled\n",
    "    fu, Delta_uu_inv = zlg.minimum_energy_solution(Delta,labeled_zlg, unlabeled_zlg,Yk_zlg)\n",
    "    \n",
    "    # score model by testing on all unlabeled points left at each step\n",
    "    scores_zlg.append(score_model(fu, Yu_zlg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "balanced-harvey",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accuracy(scores_zlg)\n",
    "print('Highest accuracy:', max(scores_zlg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chronic-pierre",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('List of queried points')\n",
    "print(queried_zlg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fallen-universe",
   "metadata": {},
   "source": [
    "### Class version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "massive-desert",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.19 ms, sys: 67 Âµs, total: 4.26 ms\n",
      "Wall time: 4.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 2 min\n",
    "\n",
    "# copy data for use in this section\n",
    "Xk_zlg2 = copy.deepcopy(Xk)\n",
    "Yk_zlg2 = copy.deepcopy(Yk)\n",
    "Xu_zlg2 = copy.deepcopy(Xu)\n",
    "Yu_zlg2 = copy.deepcopy(Yu)\n",
    "\n",
    "model = zlg.ZLG(Xk_zlg2,Yk_zlg2,Xu_zlg2,Yu_zlg2)\n",
    "queried_zlg2, scores_zlg2 = model.improve_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protective-blake",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "south-shipping",
   "metadata": {},
   "source": [
    "### Random Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appointed-smith",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time  \n",
    "# 2 min\n",
    "\n",
    "# copy data for use in this section\n",
    "Xk_rand = copy.deepcopy(Xk)\n",
    "Yk_rand = copy.deepcopy(Yk)\n",
    "Xu_rand = copy.deepcopy(Xu)\n",
    "Yu_rand = copy.deepcopy(Yu)\n",
    "\n",
    "# variables\n",
    "n_samples = X.shape[0]\n",
    "n_l = Xk_rand.shape[0]\n",
    "labeled_rand = [i for i in range(200)]\n",
    "unlabeled_rand = [i for i in range(200,673)]\n",
    "\n",
    "# initialize components\n",
    "Delta = zlg.laplacian_matrix(X,t=0)\n",
    "fu,Delta_uu_inv = zlg.minimum_energy_solution(Delta,labeled_rand,unlabeled_rand,Yk)\n",
    "\n",
    "\n",
    "# run required iterations\n",
    "queried_rand = []\n",
    "scores_rand = []\n",
    "for count in range(100):\n",
    "    \n",
    "    # select unlabeled instance \n",
    "    query_idx = np.random.randint(low=0,high=len(unlabeled_rand),size=1)[0] # random unlabeled point\n",
    "    queried_rand.append(query_idx)\n",
    "\n",
    "    # add instance to labeled set\n",
    "    Yk_rand = np.append(Yk_rand,Yu_rand[query_idx])\n",
    "    Xk_rand = np.append(Xk_rand,[Xu_rand[query_idx,:]],axis=0)\n",
    "    n_l += 1\n",
    "    labeled_rand.append(unlabeled_rand.pop(0))  # move first element in unlabeled to end of labeled list\n",
    "\n",
    "    # remove instance from unlabeled set\n",
    "    Yu_rand = np.delete(Yu_rand,query_idx)\n",
    "    Xu_rand = np.delete(Xu_rand,query_idx, 0)\n",
    "\n",
    "    # update Laplacian\n",
    "    Delta = zlg.laplacian_matrix(np.concatenate((Xk_rand,Xu_rand),axis=0), t=0)\n",
    "\n",
    "    # calculate minimum energy solution for remaining unlabeled\n",
    "    fu, Delta_uu_inv = zlg.minimum_energy_solution(Delta,labeled_rand, unlabeled_rand,Yk_rand)\n",
    "\n",
    "    # score model by testing on all unlabeled points left at each step\n",
    "    scores_rand.append(score_model(fu, Yu_rand))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recorded-television",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accuracy(scores_rand)\n",
    "print('Highest accuracy:', max(scores_rand))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "downtown-california",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('List of queried points')\n",
    "print(queried_rand)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "american-ending",
   "metadata": {},
   "source": [
    "### Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complete-underwear",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot both together\n",
    "fig, ax = plt.subplots(dpi=130)\n",
    "ax.plot(scores_zlg, label='ZLG Query')\n",
    "ax.plot(scores_rand,label='Random Query')\n",
    "plt.xlabel('Number of queries')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Number of queries vs. Accuracy')\n",
    "plt.grid()\n",
    "plt.legend()       \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greatest-circumstances",
   "metadata": {},
   "source": [
    "# 1.6. Bonus question \n",
    "\n",
    "**Answer the following questions. (Your grade will not exceed 100 for this homework.)**\n",
    "\n",
    "#### 1. For this dataset, how many labeled data points do you actually need to train the model sufficiently well? \n",
    "#### 2. And why?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
